#### Data race

##### Race Detection

As soon as we add another Goroutine to our program, we add a huge amount of complexity. We can't always let the Goroutine run stateless. There has to be coordination. There are, in fact, 2 things that we can do with multi threaded software.

- We either have to syncronize access to share state like that WaitGroup is done with Add, Done and Wait.
- Or we have to coordinate these Goroutines to behave in a predictable or responsible manner.

Up until the use of channel, we have to use atomic function, mutex, to do both. The channel gives us a simple way to do orchestration. However, in many cases, using atomic function, mutex, and synchronizing access to shared state is the best way to go.

Atomic instructions are the fastest way to go because deep down in memory, Go is syncronizing 4-8 bytes at a time.

Mutexes are the next fastest. Channels are very slow because not only they are mutexes, there are all data structures and logic that go with them. Data races happen when we have multiple Goroutines trying to access the same memory location. For example, in the simplest case, we have an integer that is a counter. We have 2 Goroutines that want to read and write to that variable at the same time. If they are actually doing it at the same time, they are going to trash each other read and write. Therefore, this type of synchronizing access to the shared state has to be coordinated.

The problem with data races is that they always appear random. Sample program to show how to create race conditions in our programs. We don't want to do this.

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
)
```

`counter` is a variable incremented by all Goroutines.

```go
var counter int

func main() {
```

Number of Goroutines to use.

```go
    const grs = 2
```

wg is used to manage concurrency.

```go
    var wg sync.WaitGroup
    wg.Add(grs)
```

Create two Goroutines.

They loop twice: perform a read to a local counter, increase by 1, write it back to the shared state. Every time we run the program, the output should be 4.
The data races that we have here is that: at any given time, both Goroutines could be reading and writing at the same time. However, we are very lucky in this case. What we are seeing is that each Goroutine is executing the 3 statements atomically completely by accident every time this code run.

If we put the line runtime.Goshed(), it will tell the scheduler to be part of the cooperation here and yield my time on that m. This will force the data race to happen. Once we read the value out of that shared state, we are gonna force the context switch. Then we come back, we are not getting 4 as frequent.

```go
    for i := 0; i < grs; i++ {
        go func() {
            for count := 0; count < 2; count++ {
```

Capture the value of Counter.

```go
                value := counter
```

Yield the thread and be placed back in queue.

FOR TESTING ONLY! DO NOT USE IN PRODUCTION CODE!

```go
                runtime.Gosched()
```

Increment our local value of Counter.

```go
                value++
```

Store the value back into Counter.

```go
                counter = value
            }
            wg.Done()
        }()
    }
```

Wait for the goroutine to finish.

```go
    wg.Wait()
    fmt.Println("Final Counter:", counter)
}
```

To identify race condition : go run -race <file_name>.

```shell
==================
WARNING: DATA RACE
Read at 0x000001228340 by goroutine 8:
main.main.func1()

/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go :65 +0x47
Previous write at 0x000001228340 by goroutine 7: main.main.func1()
/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go

:75 +0x68
Goroutine 8 (running) created at: main.main()
/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go :62 +0xab
Goroutine 7 (finished) created at: main.main()
/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go :62 +0xab
==================
Final Counter: 4
Found 1 data race(s)
exit status 66
```

##### Atomic Functions

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "sync/atomic"
)
```

counter is a variable incremented by all Goroutine. Notice that it's not just an `int` but `int64`. We are being very specific about the precision because the atomic function requires us to do so.

```go
var counter int64

func main() {
```

Number of Goroutines to use.

```go
    const grs = 2
```

wg is used to manage concurrency.

```go
    var wg sync.WaitGroup
    wg.Add(grs)
```

Create two goroutines.

```go
    for i := 0; i < grs; i++ {
        go func() {
            for count := 0; count < 2; count++ {
```

Safely add one to the counter. Add the atomic functions that we have taken an address as the first parameter and that is being syncronized, no matter how many Goroutines they are. If we call one of these functions on the same location, they will get serialized. This is the fastest way to serialization.

We can run thist program all day long and still get 4 every time.

```go
                atomic.AddInt64(&counter, 1)
```

This call is now irrelevant because by the time AddInt64 function completes, counter is increment.

```go
                runtime.Gosched()
            }

        wg.Done()
        }()
    }
```

Wait for the Goroutines to finish.

```go
    wg.Wait()
```

Display the final value.

```go
    fmt.Println("Final Counter:", counter)
}
```

```shell
Final Counter: 4
```

##### Mutexes

We don't always have the luxury of using 4-8 bytes of memory as shared data. This is where the mutex comes in. Mutex allows us to have an API like the WaitGroup (Add, Done and Wait) where any Goroutine can execute one at a time.

```go
package main

import (
    "fmt"
    "sync"
)

var (
```

counter is a variable increment by all Goroutines.

```go
    counter int
```

mutex is used to define a critical section of code. Picture mutex as a room where all Goroutines have to go through. However, only one Goroutine can go at a time. The scheduler will decide who can get in and which one is next. We cannot determine what the scheduler is gonna do. Hopefully, it is gonna be fair. Just because one Goroutine got to the door before another, it doesn't mean that Goroutine will get to the end first. Nothing here is predictable.

The key here is, once a Goroutine is allowed in, it must report that it's out. All the Goroutines will ask for a lock and unlock when they leave for another one to get in. Two different functions can use the same mutex which means only one Goroutine can execute any of given functions at a time.

```go
    mutex sync.Mutex
)
```

```go
func main() {
```

Number of Goroutines to use.

```go
    const grs = 2
```

wg is used to manage concurrency.

```go
    var wg sync.WaitGroup
    wg.Add(grs)
```

Create two Goroutines.

```go
    for i := 0; i < grs; i++ {
        go func() {
            for count := 0; count < 2; count++ {
```

Only allow one Goroutine through this critical section at a time. Creating these artificial curly brackets gives readability. We don't have to do this but it is highly recommended. The Lock and Unlock function must always be together in line of sight.

```go
                mutex.Lock()
                {
```

Capture the value of counter.

```go
                    value := counter
```

Increment our local value of counter.

```go
                    value++
```

Store the value back into counter.

```go
                    counter = value
                }
                mutex.Unlock()
```

Relese the lock and allow any waiting Goroutine through.

```go
            }

            wg.Done()
        }()
    }
```

Wait for the Goroutines to finish.

```go
    wg.Wait()
    fmt.Printf("Final Counter: %d\n", counter)
}
```

```shell
Final Counter: 4
```

##### Read/Write Mutex

There are times when we have a shared resource where we want many Goroutines reading it.

Occasionally, one Goroutine can come in and make change to the resource. When that happens, everybody has to stop reading. It doesn't make sense to synchronize reads in this type of scenario because we are just adding latency to our software for no reason.

```go
package main

import (
    "fmt"
    "math/rand"
    "sync"
    "sync/atomic"
    "time"
)
```

data is a slice that will be shared.

```go
var (
    data []string
```

`rwMutex` is used to define a critical section of code. It is a little bit slower than Mutex but we are optimizing the correctness first so we don't care about that for now.

```go
    rwMutex sync.RWMutex
```

Number of reads occurring at any given time. As soon as we see `int64` here, we should start thinking about using atomic instruction.

```go
    readCount int64
)
```

`init` is called prior to `main`.

```go
func init() {
    rand.Seed(time.Now().UnixNano())
}

func main() {
```

wg is used to manage concurrency.

```go
    var wg sync.WaitGroup
    wg.Add(1)
```

Create a writer Goroutine that performs 10 different writes.

```go
    go func() {
        for i := 0; i < 10; i++ {
            time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
            writer(i)
        }
        wg.Done()
    }()
```

Create eight reader Goroutines that runs forever.

```go
    for i := 0; i < 8; i ++ {
        go func(i int) {
            for {
                reader(i)
            }
        }(i)
    }
```

Wait for the write Goroutine to finish.

```go
    wg.Wait()
    fmt.Println("Program Complete")
}
```

writer adds a new string to the slice in random intervals.

```go
func writer(i int) {
```

Only allow one Goroutine to read/write to the slice at a time.

```go
    rwMutex.Lock()
    {
```

Capture the current read count. Keep this safe though we can do it without this call. We want to make sure that no other Goroutines are reading. The value of rc should always be 0 when this code runs.

```go
        rc := atomic.LoadInt64(&readCount)
```

Perform some work since we have a full lock.

```go
        fmt.Printf("****> : Performing Write : RCount[%d]\n", rc)
        data = append(data, fmt.Sprintf("String: %d", i))
    }
    rwMutex.Unlock()
}
```

reader wakes up and iterates over the data slice.

```go
func reader(id int) {
```

Any Goroutine can read when no write operation is taking place. RLock has the corresponding RUnlock.

```go
    rwMutex.RLock()
    {
```

Increment the read count value by 1.

```go
        rc := atomic.AddInt64(&readCount, 1)
```

Perform some read work and display values.

```go
        time.Sleep(time.Duration(rand.Intn(10)) * time.Millisecond)
        fmt.Printf("%d : Performing Read : Length[%d] RCount[%d]\n", id, len(data), rc)
```

Decrement the read count value by 1.

```go
        atomic.AddInt64(&readCount, -1)
    }
    rwMutex.RUnlock()
}
```

The output will lock similar to this.

```shell
0 : Performing Read : Length[0] RCount[1]
4 : Performing Read : Length[0] RCount[5]
5 : Performing Read : Length[0] RCount[6]
7 : Performing Read : Length[0] RCount[7]
3 : Performing Read : Length[0] RCount[4]
6 : Performing Read : Length[0] RCount[8]
4 : Performing Read : Length[0] RCount[8]
1 : Performing Read : Length[0] RCount[2]
2 : Performing Read : Length[0] RCount[3]
5 : Performing Read : Length[0] RCount[8]
0 : Performing Read : Length[0] RCount[8]
7 : Performing Read : Length[0] RCount[8]
7 : Performing Read : Length[0] RCount[8]
2 : Performing Read : Length[0] RCount[8]
...
1 : Performing Read : Length[10] RCount[8]
5 : Performing Read : Length[10] RCount[8]
3 : Performing Read : Length[10] RCount[8]
4 : Performing Read : Length[10] RCount[8]
6 : Performing Read : Length[10] RCount[8]
7 : Performing Read : Length[10] RCount[8]
2 : Performing Read : Length[10] RCount[8]
2 : Performing Read : Length[10] RCount[8]
```

**Lesson:**

The atomic functions and mutexes create latency in our software. Latency can be good when we have to coordinate orchestrating. However, if we can reduce latency using Read/Write Mutex, life is better.

If we are using mutex, make sure that we get in and out of mutex as fast as possible. Do not do anything extra. Sometimes just reading the shared state into a local variable is all we need to do. The less operation we can perform on the mutex, the better. We then reduce the latency to the bare minimum.
