#### 자원 경쟁(Data race)

##### 경쟁 감지(Race Detection)

프로그램에 고루틴을 추가하면 복잡도가 엄청나게 올라간다. 고루틴을 언제나 상태없이(stateless) 실행할 수는 없기에 조율이 필요하다. 멀티 스레드 소프트웨어를 작성할 때는 사실상 두 가지 선택지가 있다.

- WaitGroup에서 Add와 Done, Wait으로 제어하는 것 처럼, 공유 자원에 대한 접근 상태를 동기화하거나
- 고루틴을 예측 가능하고, 합리적으로 실행이 되도록 만들어야 한다.

채널이 없었을 때는, 아토믹 함수나 mutex를 사용하여 앞서 언급한 두 가지 선택지를 구현하였다. 채널은 간단한 제어 방법을 제공하지만, 대부분의 경우는 아토믹 함수와 mutex를 사용하여 공유 자원에 대한 액세스 동기화를 사용하는 것이 가장 좋은 방법이다.
atomic 연산은 Go에서 가장 빠른 방법이다. Go는 메모리에서 한번에 4-8 바이트씩 동기화를 하기 때문이다.

Mutex는 다음으로 빠르다. 채널은 매우 느린데, mutex일 뿐만 아니라 모든 데이터 구조와 로직이 함께 있기 때문이다. 여러개의 고루틴이 같은 자원에 접근하려할 때 자원 경쟁이 발생한다. 예를 들어, 2개의 고루틴이 int 타입의 counter라는 변수에 같은 시각에 접근하길 원하는 상황을 가정해 본다. 만약 실제로 같은 시간에 접근한다면 읽고 쓰기 위해 상호배제 할 것이다. 그렇기 때문에 공유하는 자원에 대해서 이런 접근이 필요할 때는 조정이 필요하다.

진짜 문제는 이러한 자원 경쟁이 항상 예상치 못하게 나타난다는 것이다. 예시 프로그램을 통해, 우리가 원하지 않는 자원 경쟁 상태를 만들어서 확인한다.

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
)
```

`counter`는 모든 고루틴에 의해 증가되는 변수이다.

```go
var counter int

func main() {
```

사용할 고루틴의 수.

```go
    const grs = 2
```

`wg`는 동시성을 관리하는데 사용된다.

```go
    var wg sync.WaitGroup
    wg.Add(grs)
```

2개의 고루틴을 만들어준다.

두번 반복: local `counter`에 읽기를 수행하고 1 씩 증가한 다음 공유 상태에 다시 쓴다. 프로그램을 실행할 때마다 출력은 4가되어야한다.
여기서 발생하는 자원 경쟁: 주어진 시간동안 두개의 고루틴은 동시에 읽고, 쓸 수 있다. 그러나 우리는 운이 좋게도 각각의 고루틴이 3번의 실행을 모두 atomic하게 실행하고있다는 것을 볼 수 있다.

만약 `runtime.Goshed()`라는 줄을 추가하게되면, 다른 고루틴에게 CPU를 양보하게된다. 이때, 공유 자원을 읽게되면, 강제로 context switch가 일어나게되고, 자원 경쟁이 발생할 수 있다. 그렇게 되면 다시 돌아왔을 때 4라는 결과값을 얻지 못할수도 있다.

```go
    for i := 0; i < grs; i++ {
        go func() {
            for count := 0; count < 2; count++ {
```

이 때의 `counter` 값을 저장해 둔다.

```go
                value := counter
```

다른 고루틴에게 스레드를 양보하고, 다시 대기열에 들어간다.

FOR TESTING ONLY! DO NOT USE IN PRODUCTION CODE!

```go
                runtime.Gosched()
```

`counter`의 값을 늘린다.

```go
                value++
```

값을 `counter`에 다시 저장한다.

```go
                counter = value
            }
            wg.Done()
        }()
    }
```

고루틴이 끝날 때까지 기다린다.

```go
    wg.Wait()
    fmt.Println("Final Counter:", counter)
}
```

To identify race condition : go run -race <file_name>.

```shell
==================
WARNING: DATA RACE
Read at 0x000001228340 by goroutine 8:
main.main.func1()

/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go :65 +0x47
Previous write at 0x000001228340 by goroutine 7: main.main.func1()
/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go

:75 +0x68
Goroutine 8 (running) created at: main.main()
/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go :62 +0xab
Goroutine 7 (finished) created at: main.main()
/Users/hoanhan/work/hoanhan101/ultimate-go/go/concurrency/data_race_1.go :62 +0xab
==================
Final Counter: 4
Found 1 data race(s)
exit status 66
```

##### Atomic Functions

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "sync/atomic"
)
```

`counter`는 모든 고루틴에 의해 증가되는 변수이다. 해당 변수가 `int`가 아닌 `int64` 타입이라는 것에 유의해야한다. 아토믹 함수의 경우 정확성을 요구하기 때문에 구체적인 타입을 명시해야한다.

```go
var counter int64

func main() {
```

Number of Goroutines to use.

```go
    const grs = 2
```

wg is used to manage concurrency.

```go
    var wg sync.WaitGroup
    wg.Add(grs)
```

Create two goroutines.

```go
    for i := 0; i < grs; i++ {
        go func() {
            for count := 0; count < 2; count++ {
```

Safely add one to the counter. Add the atomic functions that we have taken an address as the first parameter and that is being syncronized, no matter how many Goroutines they are. If we call one of these functions on the same location, they will get serialized. This is the fastest way to serialization.

We can run thist program all day long and still get 4 every time.

```go
                atomic.AddInt64(&counter, 1)
```

This call is now irrelevant because by the time AddInt64 function completes, counter is increment.

```go
                runtime.Gosched()
            }
        wg.Done()
        }()
    }
```

Wait for the Goroutines to finish.

```go
    wg.Wait()
```

Display the final value.

```go
    fmt.Println("Final Counter:", counter)
}
```

```shell
Final Counter: 4
```

##### Mutexes

We don't always have the luxury of using 4-8 bytes of memory as shared data. This is where the mutex comes in. Mutex allows us to have an API like the WaitGroup (Add, Done and Wait) where any Goroutine can execute one at a time.

```go
package main

import (
    "fmt"
    "sync"
)

var (
```

counter is a variable increment by all Goroutines.

```go
    counter int
```

mutex is used to define a critical section of code. Picture mutex as a room where all Goroutines have to go through. However, only one Goroutine can go at a time. The scheduler will decide who can get in and which one is next. We cannot determine what the scheduler is gonna do. Hopefully, it is gonna be fair. Just because one Goroutine got to the door before another, it doesn't mean that Goroutine will get to the end first. Nothing here is predictable.

The key here is, once a Goroutine is allowed in, it must report that it's out. All the Goroutines will ask for a lock and unlock when they leave for another one to get in. Two different functions can use the same mutex which means only one Goroutine can execute any of given functions at a time.

```go
    mutex sync.Mutex
)
```

```go
func main() {
```

Number of Goroutines to use.

```go
    const grs = 2
```

wg is used to manage concurrency.

```go
    var wg sync.WaitGroup
    wg.Add(grs)
```

Create two Goroutines.

```go
    for i := 0; i < grs; i++ {
        go func() {
            for count := 0; count < 2; count++ {
```

Only allow one Goroutine through this critical section at a time. Creating these artificial curly brackets gives readability. We don't have to do this but it is highly recommended. The Lock and Unlock function must always be together in line of sight.

```go
                mutex.Lock()
                {
```

Capture the value of counter.

```go
                    value := counter
```

Increment our local value of counter.

```go
                    value++
```

Store the value back into counter.

```go
                    counter = value
                }
                mutex.Unlock()
```

Relese the lock and allow any waiting Goroutine through.

```go
            }

            wg.Done()
        }()
    }
```

Wait for the Goroutines to finish.

```go
    wg.Wait()
    fmt.Printf("Final Counter: %d\n", counter)
}
```

```shell
Final Counter: 4
```

##### Read/Write Mutex

There are times when we have a shared resource where we want many Goroutines reading it.

Occasionally, one Goroutine can come in and make change to the resource. When that happens, everybody has to stop reading. It doesn't make sense to synchronize reads in this type of scenario because we are just adding latency to our software for no reason.

```go
package main

import (
    "fmt"
    "math/rand"
    "sync"
    "sync/atomic"
    "time"
)
```

data is a slice that will be shared.

```go
var (
    data []string
```

`rwMutex` is used to define a critical section of code. It is a little bit slower than Mutex but we are optimizing the correctness first so we don't care about that for now.

```go
    rwMutex sync.RWMutex
```

Number of reads occurring at any given time. As soon as we see `int64` here, we should start thinking about using atomic instruction.

```go
    readCount int64
)
```

`init` is called prior to `main`.

```go
func init() {
    rand.Seed(time.Now().UnixNano())
}

func main() {
```

wg is used to manage concurrency.

```go
    var wg sync.WaitGroup
    wg.Add(1)
```

Create a writer Goroutine that performs 10 different writes.

```go
    go func() {
        for i := 0; i < 10; i++ {
            time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
            writer(i)
        }
        wg.Done()
    }()
```

Create eight reader Goroutines that runs forever.

```go
    for i := 0; i < 8; i ++ {
        go func(i int) {
            for {
                reader(i)
            }
        }(i)
    }
```

Wait for the write Goroutine to finish.

```go
    wg.Wait()
    fmt.Println("Program Complete")
}
```

writer adds a new string to the slice in random intervals.

```go
func writer(i int) {
```

Only allow one Goroutine to read/write to the slice at a time.

```go
    rwMutex.Lock()
    {
```

Capture the current read count. Keep this safe though we can do it without this call. We want to make sure that no other Goroutines are reading. The value of rc should always be 0 when this code runs.

```go
        rc := atomic.LoadInt64(&readCount)
```

Perform some work since we have a full lock.

```go
        fmt.Printf("****> : Performing Write : RCount[%d]\n", rc)
        data = append(data, fmt.Sprintf("String: %d", i))
    }
    rwMutex.Unlock()
}
```

reader wakes up and iterates over the data slice.

```go
func reader(id int) {
```

Any Goroutine can read when no write operation is taking place. RLock has the corresponding RUnlock.

```go
    rwMutex.RLock()
    {
```

Increment the read count value by 1.

```go
        rc := atomic.AddInt64(&readCount, 1)
```

Perform some read work and display values.

```go
        time.Sleep(time.Duration(rand.Intn(10)) * time.Millisecond)
        fmt.Printf("%d : Performing Read : Length[%d] RCount[%d]\n", id, len(data), rc)
```

Decrement the read count value by 1.

```go
        atomic.AddInt64(&readCount, -1)
    }
    rwMutex.RUnlock()
}
```

The output will lock similar to this.

```shell
0 : Performing Read : Length[0] RCount[1]
4 : Performing Read : Length[0] RCount[5]
5 : Performing Read : Length[0] RCount[6]
7 : Performing Read : Length[0] RCount[7]
3 : Performing Read : Length[0] RCount[4]
6 : Performing Read : Length[0] RCount[8]
4 : Performing Read : Length[0] RCount[8]
1 : Performing Read : Length[0] RCount[2]
2 : Performing Read : Length[0] RCount[3]
5 : Performing Read : Length[0] RCount[8]
0 : Performing Read : Length[0] RCount[8]
7 : Performing Read : Length[0] RCount[8]
7 : Performing Read : Length[0] RCount[8]
2 : Performing Read : Length[0] RCount[8]
...
1 : Performing Read : Length[10] RCount[8]
5 : Performing Read : Length[10] RCount[8]
3 : Performing Read : Length[10] RCount[8]
4 : Performing Read : Length[10] RCount[8]
6 : Performing Read : Length[10] RCount[8]
7 : Performing Read : Length[10] RCount[8]
2 : Performing Read : Length[10] RCount[8]
2 : Performing Read : Length[10] RCount[8]
```

**Lesson:**

The atomic functions and mutexes create latency in our software. Latency can be good when we have to coordinate orchestrating. However, if we can reduce latency using Read/Write Mutex, life is better.

If we are using mutex, make sure that we get in and out of mutex as fast as possible. Do not do anything extra. Sometimes just reading the shared state into a local variable is all we need to do. The less operation we can perform on the mutex, the better. We then reduce the latency to the bare minimum.
